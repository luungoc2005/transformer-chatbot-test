{"model_type": "transformer", "emb_size": 256, "ninp": 256, "nhid": 1024, "nlayers": 6, "nhead": 4, "dropout": 0.1, "tie_encoder_decoder": false, "tie_layers": true, "lr": 0.0056, "min_lr": 0, "adam_eps": 1e-09, "num_warmup_steps": 3125, "weight_decay": 0.01, "batch_size": 64, "accumulate_grad_batches": 2, "bptt": 128}